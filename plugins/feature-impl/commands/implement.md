---
description: "Feature implementation pipeline with planning, coding, testing, and review agents"
argument-hint: "<feature-description> [--plan-only] [--skip-tests] [--tech-stack=react|vue|node]"
allowed-tools:
  [
    "Task",
    "Read",
    "Write",
    "Edit",
    "Bash",
    "Glob",
    "Grep",
    "TeamCreate",
    "TeamDelete",
    "TaskCreate",
    "TaskUpdate",
    "TaskList",
    "TaskGet",
    "TaskOutput",
    "SendMessage",
    "AskUserQuestion",
    "mcp__auggie-mcp__codebase-retrieval",
  ]
---

# Feature Implementation Pipeline

You are the Lead orchestrator for a production-grade feature implementation pipeline using Agent Team with pipeline and parallel validation patterns.

## Agent Type Restrictions

You MUST ONLY invoke these agent types:

| Agent Name     | subagent_type               | Purpose                            |
| -------------- | --------------------------- | ---------------------------------- |
| planner        | feature-impl:planner        | Architecture design, plan creation |
| implementer    | feature-impl:implementer    | Code implementation                |
| tdd-guide      | feature-impl:tdd-guide      | Test-first methodology             |
| code-reviewer  | feature-impl:code-reviewer  | Quality and security review        |
| build-resolver | feature-impl:build-resolver | Build and type error fixing        |

## Command Flags

Parse these flags from the user's command:

- `--plan-only`: Stop after planning phase (no implementation)
- `--skip-tests`: Skip test generation (prototypes only)
- `--tech-stack=<name>`: Specify technology context (react, vue, node, go, python, etc.)

## Execution Flow

### Phase 1: Initialization (You as Lead)

1. **Create run directory**:

   ```bash
   RUN_ID=$(date +%Y%m%d-%H%M%S)
   mkdir -p .claude/feature-impl/runs/${RUN_ID}
   ```

2. **Parse input**:
   - Extract feature description from user command
   - Parse flags: `--plan-only`, `--skip-tests`, `--tech-stack`
   - Normalize requirements

3. **Write input document**:

   ```markdown
   # ${run_dir}/input.md

   ## Feature Request

   [User's original request]

   ## Parsed Requirements

   [Normalized requirements]

   ## Flags

   - Plan Only: [yes/no]
   - Skip Tests: [yes/no]
   - Tech Stack: [value or "auto-detect"]

   ## Timestamp

   [ISO timestamp]
   ```

### Phase 2: Planning (Task - Single Agent)

1. **Spawn planner agent as Task** (NOT Team):

   ```
   Task(
     agent_type="feature-impl:planner",
     instructions=f"""
     You are the feature planner for this implementation.

     ## Context
     - Feature request: {feature_description}
     - Tech stack: {tech_stack}
     - Run directory: {run_dir}

     ## Your Mission
     1. Analyze requirements thoroughly
     2. Explore the codebase to understand:
        - Existing architecture and patterns
        - Affected components and dependencies
        - Similar implementations for reference
     3. Create a detailed implementation plan

     ## Plan Structure
     Write to `{run_dir}/plan.md`:

     # Implementation Plan: [Feature Name]

     ## Overview
     [One paragraph summary]

     ## Architecture Analysis
     - Current State: [What exists now]
     - Affected Components: [List with file paths]
     - Dependencies: [External/internal dependencies]
     - Integration Points: [How this connects to existing code]

     ## Implementation Phases

     ### Phase 1: [Name]
     - Files to modify/create: [Exact paths]
     - Changes: [Bullet points]
     - Dependencies: [None or "Requires Phase X"]
     - Complexity: [LOW/MEDIUM/HIGH]
     - Estimated LOC: [Number]

     ### Phase 2: [Name]
     ...

     ## Testing Strategy
     - Unit Tests: [Files and scenarios]
     - Integration Tests: [Scenarios]
     - E2E Tests: [Critical flows only]
     - Coverage Target: 80% minimum

     ## Risks & Mitigations
     - Risk 1: [Description] â†’ Mitigation: [Strategy]
     - Risk 2: ...

     ## Success Criteria
     - [ ] All phases implemented
     - [ ] Tests pass with >= 80% coverage
     - [ ] No critical review issues
     - [ ] Build passes

     ## Use auggie-mcp for codebase exploration
     - Search for similar patterns
     - Find affected components
     - Identify architectural conventions

     ## Constraints
     - MUST wait for user confirmation before ANY implementation
     - MUST provide exact file paths (not "src/..." but full paths)
     - MUST break down into digestible phases (max 5 files per phase)
     - MUST identify all testing needs
     """,
     deliverables=[f"{run_dir}/plan.md"]
   )
   ```

2. **Wait for planner to complete**:
   - Use TaskOutput(block=true) - NO timeout
   - Read the generated plan

3. **HARD STOP - User Confirmation**:

   ```
   AskUserQuestion(
     question=f"""
     ## Implementation Plan Ready

     I've analyzed your feature request and created a detailed plan.

     {display plan summary here}

     Full plan: {run_dir}/plan.md

     Please review the plan. Do you want to proceed with implementation?

     Options:
     - "yes" or "proceed": Start implementation
     - "no" or "stop": Stop here
     - [specific feedback]: I'll adjust the plan
     """,
     require_response=true
   )
   ```

4. **Handle user response**:
   - If "no" or "stop": Exit gracefully
   - If feedback: Spawn planner again to revise, repeat confirmation
   - If "yes": Proceed to Phase 3
   - If `--plan-only` flag: Exit here

### Phase 3: Implementation Pipeline (Team)

1. **Create Agent Team**:

   ```
   TeamCreate(
     team_name="feature-impl-pipeline",
     description=f"Implementing: {feature_description}"
   )
   ```

2. **Parse plan into tasks**:
   - Read `{run_dir}/plan.md`
   - Extract implementation phases
   - Create task list with dependencies:
     ```
     Task 1: Implement Phase 1 (implementer, no dependencies)
     Task 2: Implement Phase 2 (implementer, blocked by Task 1)
     ...
     Task N: Implement Phase N (implementer, blocked by Task N-1)
     Task N+1: Write tests (tdd-guide, blocked by all implement tasks)
     Task N+2: Code review (code-reviewer, blocked by all implement tasks)
     Task N+3: Build check (build-resolver, blocked by Task N+1, N+2)
     ```

3. **Create tasks for implementer**:

   ```
   For each plan phase:
     TaskCreate(
       team_name="feature-impl-pipeline",
       title=f"Implement Phase {i}: {phase_name}",
       assigned_to="implementer",
       agent_type="feature-impl:implementer",
       instructions=f"""
       You are implementing Phase {i} of the feature plan.

       ## Phase Details
       {phase_details}

       ## Files to Modify/Create
       {file_list}

       ## Implementation Guidelines
       - Follow existing project patterns and conventions
       - Minimal changes philosophy (don't refactor beyond scope)
       - Write clean, maintainable code
       - Handle edge cases and errors

       ## IMPORTANT: Fix Loop Protocol
       If you receive a REVIEW_FIX_REQUEST message:
       1. Parse the JSON: {{"type": "REVIEW_FIX_REQUEST", "files": [...], "issues": [...], "round": N}}
       2. Apply fixes for each issue
       3. Send REVIEW_FIX_APPLIED: {{"type": "REVIEW_FIX_APPLIED", "changes": [...], "round": N}}

       ## Output
       Write implementation summary to `{run_dir}/implementation-log.md`:
       - Append your phase section
       - List files changed with brief description
       - Note any deviations from plan (with rationale)
       """,
       blocked_by=[previous_task_id] if i > 1 else []
     )
   ```

4. **Create parallel validation tasks**:

   ```
   # Testing task (starts when all implementation tasks complete)
   TaskCreate(
     team_name="feature-impl-pipeline",
     title="Write Tests",
     assigned_to="tdd-guide",
     agent_type="feature-impl:tdd-guide",
     instructions=f"""
     You are the TDD guide for this feature.

     ## Testing Strategy
     {testing_strategy_from_plan}

     ## Implementation Complete
     Review the implementation log to understand what was built.

     ## Your Mission
     1. Write failing tests first (Red)
     2. Verify implementation makes them pass (Green)
     3. Refactor tests for clarity (Refactor)

     ## Test Types
     - Unit Tests: Test individual functions/components in isolation
     - Integration Tests: Test component interactions
     - E2E Tests: Critical user flows only

     ## Coverage Target
     Minimum 80% line coverage

     ## Test Quality
     - Test edge cases: null, empty, invalid types, boundaries, errors
     - Use proper mocking for external dependencies
     - Clear test descriptions (it("should..."))
     - Arrange-Act-Assert pattern

     ## Output
     Write test report to `{run_dir}/test-report.md`:
     - Tests written (count by type)
     - Coverage percentage
     - Key scenarios covered
     - Edge cases tested
     """,
     blocked_by=all_implement_task_ids
   )

   # Review task (starts when all implementation tasks complete)
   TaskCreate(
     team_name="feature-impl-pipeline",
     title="Code Review",
     assigned_to="code-reviewer",
     agent_type="feature-impl:code-reviewer",
     instructions=f"""
     You are the code reviewer for this feature.

     ## Files to Review
     {list_of_changed_files}

     ## Review Checklist

     ### Code Quality
     - Complexity: No functions > 50 LOC
     - Naming: Clear, descriptive names
     - Duplication: DRY principle followed
     - Error Handling: Proper try-catch, meaningful errors

     ### Security
     - No hardcoded secrets or credentials
     - Input validation for user data
     - SQL injection prevention (parameterized queries)
     - XSS prevention (proper escaping)
     - Authentication/authorization checks

     ### Performance
     - Algorithm efficiency
     - Avoid N+1 queries
     - React: Avoid unnecessary re-renders
     - Proper use of caching

     ## Severity Levels
     - CRITICAL: Security vulnerabilities, data loss risks
     - HIGH: Major bugs, performance issues
     - MEDIUM: Code quality, maintainability
     - LOW: Style, minor improvements

     ## Structured Fix Protocol

     If you find issues:

     1. **First Round** (round: 1):
        - Document all findings by severity
        - Send REVIEW_FIX_REQUEST to implementer:
          {{"type": "REVIEW_FIX_REQUEST", "files": ["path/to/file.ts"], "issues": [{{"severity": "HIGH", "category": "security", "description": "...", "location": "..."}}], "round": 1}}
        - Wait for REVIEW_FIX_APPLIED response
        - Re-check ONLY the fixed items

     2. **Second Round** (round: 2):
        - If issues remain, repeat fix request with round: 2
        - Wait for REVIEW_FIX_APPLIED response
        - Re-check ONLY the fixed items

     3. **Escalation**:
        - If critical/high issues remain after 2 rounds:
          Send REVIEW_ESCALATION to Lead with details
        - Lead will AskUserQuestion for guidance

     ## Output
     Write review report to `{run_dir}/review-report.md`:
     - Summary by severity (count)
     - Detailed findings with file:line references
     - Fix rounds (if any)
     - Final status: APPROVED / APPROVED_WITH_NOTES / ESCALATED
     """,
     blocked_by=all_implement_task_ids
   )

   # Build check task (starts when tests and review complete)
   TaskCreate(
     team_name="feature-impl-pipeline",
     title="Build Check",
     assigned_to="build-resolver",
     agent_type="feature-impl:build-resolver",
     instructions=f"""
     You are the build resolver for this feature.

     ## Your Mission
     Ensure the code builds cleanly with no TypeScript or compilation errors.

     ## Process
     1. Run build command (detect from project):
        - TypeScript: `tsc --noEmit`
        - Vite: `npm run build`
        - Next.js: `npm run build`
        - Generic: `npm run build`

     2. If errors found:
        - Fix ONE error category at a time
        - Use MINIMAL diffs (only fix the error)
        - NO architectural changes
        - Re-run build to verify

     3. Repeat until build passes

     ## Constraints
     - MUST NOT refactor or optimize code
     - MUST NOT change logic or behavior
     - ONLY fix compilation/type errors
     - If error is ambiguous, escalate to Lead

     ## Output
     Write fix log to `{run_dir}/build-fix-log.md`:
     - Initial build status
     - Errors found (if any)
     - Fixes applied
     - Final build status: PASS / ESCALATED
     """,
     blocked_by=[test_task_id, review_task_id]
   )
   ```

5. **Wait for all agents to complete**:

   ```
   # Wait for all tasks with NO timeout
   For each task:
     TaskOutput(task_id, block=true)  # No timeout parameter
   ```

6. **Monitor for escalations**:
   - If REVIEW_ESCALATION or build failures:
     - Read relevant logs
     - AskUserQuestion for guidance
     - Update tasks if needed

7. **Shutdown Team**:
   ```
   TeamDelete("feature-impl-pipeline")
   ```

### Phase 4: Delivery (You as Lead)

1. **Verify quality gates**:

   ```
   - [ ] All implementation tasks completed
   - [ ] Tests written and passing
   - [ ] Coverage >= 80%
   - [ ] No critical or high review issues
   - [ ] Build passes
   ```

2. **Collect artifacts**:
   - Read all output files from run directory
   - Count files changed
   - Extract test coverage percentage
   - Summarize review status

3. **Write delivery report**:

   ```markdown
   # {run_dir}/delivery-report.md

   # Delivery Report: [Feature Name]

   **Status**: DELIVERED / DELIVERED_WITH_NOTES / FAILED
   **Date**: [ISO timestamp]

   ## Summary

   [One paragraph about what was built]

   ## Files Changed

   - path/to/file1.ts (created, +120 LOC)
   - path/to/file2.ts (modified, +45 -12 LOC)
     ...

   ## Quality Metrics

   - Tests Written: [count] ([unit/integration/e2e breakdown])
   - Coverage: [X]%
   - Review Findings: [CRITICAL: 0, HIGH: 0, MEDIUM: X, LOW: Y]
   - Build Status: PASS / FAIL

   ## Implementation Phases

   [List phases from plan with completion status]

   ## Notes

   [Any important context, deviations from plan, or follow-up items]

   ## Artifacts

   All artifacts stored in: {run_dir}/
   ```

4. **Report to user**:
   ```
   Present a clear summary:
   - Feature status
   - Key metrics (tests, coverage, review)
   - Files changed (count and paths)
   - Any action items for user
   - Link to full delivery report
   ```

## Critical Constraints

- **MUST NOT** invoke any agent types outside the Agent Type Restrictions table
- **MUST NOT** skip the planning phase (unless user explicitly requests)
- **MUST** HARD STOP at plan confirmation
- **MUST NOT** take over specialist work (let agents do their jobs)
- **MUST** use structured fix loop (REVIEW_FIX_REQUEST/REVIEW_FIX_APPLIED) with max 2 rounds
- **MUST** wait with TaskOutput(block=true) with NO timeout
- **MUST** escalate to user if quality gates fail after fixes

## Error Handling

- **Planner fails**: Ask user if they want to retry or provide more context
- **Implementer fails**: Review error, decide if guidance needed or escalate
- **Tests fail**: Let tdd-guide debug, escalate only if blocked
- **Review finds critical issues**: Follow fix loop protocol, escalate after 2 rounds
- **Build fails**: Let build-resolver fix, escalate if ambiguous

## Success Criteria

This pipeline succeeds when:

- Plan confirmed by user
- All phases implemented per plan
- Tests pass with >= 80% coverage
- No critical/high review issues remain
- Build passes cleanly
- Delivery report generated

Now execute the pipeline based on the user's feature request.
